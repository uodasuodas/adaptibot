{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Object Detection Debugging\n",
    "Simplified notebook for testing detection models with interactive controls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.6.0\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (12.4.127)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ut-ai/jupyter_env/lib/python3.12/site-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.1.0\n",
      "\u001b[2K    Uninstalling triton-3.1.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.1.0\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: torch 2.5.1\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling torch-2.5.1:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.5.1m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.6.0 triton-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded google/owlvit-base-patch16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded google/owlvit-base-patch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded google/owlvit-large-patch14\n",
      " Loaded GroundingDINO\n"
     ]
    }
   ],
   "source": [
    "# Detection Models Setup\n",
    "class DetectionManager:\n",
    "    def __init__(self):\n",
    "        self.detectors = {}\n",
    "        self.available_models = []\n",
    "        self.setup_models()\n",
    "    \n",
    "    def create_owl_detector(self, model_name):\n",
    "        \"\"\"Create OWL-ViT detector\"\"\"\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        detector = pipeline(\n",
    "            \"zero-shot-object-detection\",\n",
    "            model=model_name,\n",
    "            device=device,\n",
    "            torch_dtype=torch.float16 if device >= 0 else torch.float32,\n",
    "        )\n",
    "        return detector\n",
    "    \n",
    "    def create_grounding_dino(self):\n",
    "        \"\"\"Create GroundingDINO detector\"\"\"\n",
    "        try:\n",
    "            processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "            model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "            \n",
    "            def grounding_detect(image_path, candidate_labels):\n",
    "                image = Image.open(image_path)\n",
    "                text_prompt = \" . \".join(candidate_labels[:15]) + \" .\"\n",
    "                inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                # Get image size for post-processing\n",
    "                target_sizes = torch.tensor([image.size[::-1]])\n",
    "                \n",
    "                # Fixed post-processing call - remove unsupported arguments\n",
    "                try:\n",
    "                    # Try with threshold arguments (newer versions)\n",
    "                    results = processor.post_process_grounded_object_detection(\n",
    "                        outputs, inputs.input_ids, target_sizes=target_sizes,\n",
    "                        box_threshold=0.1, text_threshold=0.1\n",
    "                    )[0]\n",
    "                except TypeError:\n",
    "                    # Fallback for older versions without threshold arguments\n",
    "                    results = processor.post_process_grounded_object_detection(\n",
    "                        outputs, inputs.input_ids, target_sizes=target_sizes\n",
    "                    )[0]\n",
    "                \n",
    "                detections = []\n",
    "                if \"boxes\" in results and \"scores\" in results and \"labels\" in results:\n",
    "                    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
    "                        # Apply manual threshold filtering\n",
    "                        if float(score) >= 0.1:\n",
    "                            # Handle both string labels and numeric indices\n",
    "                            if isinstance(label, str):\n",
    "                                # GroundingDINO returns text labels directly\n",
    "                                label_text = label.strip()\n",
    "                            else:\n",
    "                                # Fallback to index-based lookup\n",
    "                                label_idx = int(label)\n",
    "                                if label_idx < len(candidate_labels):\n",
    "                                    label_text = candidate_labels[label_idx]\n",
    "                                else:\n",
    "                                    label_text = \"object\"\n",
    "                            \n",
    "                            detections.append({\n",
    "                                \"box\": {\"xmin\": float(box[0]), \"ymin\": float(box[1]), \n",
    "                                       \"xmax\": float(box[2]), \"ymax\": float(box[3])},\n",
    "                                \"score\": float(score),\n",
    "                                \"label\": label_text\n",
    "                            })\n",
    "                \n",
    "                return detections\n",
    "            \n",
    "            return grounding_detect\n",
    "        except Exception as e:\n",
    "            print(f\"GroundingDINO not available: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Setup all available models\"\"\"\n",
    "        owl_models = [\n",
    "            \"google/owlvit-base-patch16\",\n",
    "            \"google/owlvit-base-patch32\", \n",
    "            \"google/owlvit-large-patch14\"\n",
    "        ]\n",
    "        \n",
    "        for model_name in owl_models:\n",
    "            try:\n",
    "                self.detectors[model_name] = self.create_owl_detector(model_name)\n",
    "                self.available_models.append(model_name)\n",
    "                print(f\" Loaded {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\" Failed to load {model_name}: {e}\")\n",
    "        \n",
    "        # Try GroundingDINO\n",
    "        grounding_detector = self.create_grounding_dino()\n",
    "        if grounding_detector:\n",
    "            self.detectors[\"GroundingDINO\"] = grounding_detector\n",
    "            self.available_models.append(\"GroundingDINO\")\n",
    "            print(\" Loaded GroundingDINO\")\n",
    "    \n",
    "    def detect(self, model_name, image_path, search_terms):\n",
    "        \"\"\"Run detection with specified model\"\"\"\n",
    "        if model_name not in self.detectors:\n",
    "            raise ValueError(f\"Model {model_name} not available\")\n",
    "        \n",
    "        detector = self.detectors[model_name]\n",
    "        \n",
    "        if model_name == \"GroundingDINO\":\n",
    "            return detector(image_path, search_terms)\n",
    "        else:\n",
    "            return detector(image_path, candidate_labels=search_terms)\n",
    "\n",
    "# Initialize detection manager\n",
    "detector_manager = DetectionManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_detections(image, detections, title=\"Detections\", confidence_threshold=0.1):\n",
    "    \"\"\"Visualize detections with bounding boxes\"\"\"\n",
    "    img_vis = image.copy()\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), \n",
    "              (255, 0, 255), (0, 255, 255), (255, 128, 0), (128, 255, 0)]\n",
    "    \n",
    "    filtered_dets = [d for d in detections if d[\"score\"] >= confidence_threshold]\n",
    "    label_to_color = {}\n",
    "    color_idx = 0\n",
    "    \n",
    "    for det in filtered_dets:\n",
    "        label = det[\"label\"]\n",
    "        score = det[\"score\"]\n",
    "        box = det[\"box\"]\n",
    "        \n",
    "        if label not in label_to_color:\n",
    "            label_to_color[label] = colors[color_idx % len(colors)]\n",
    "            color_idx += 1\n",
    "        color = label_to_color[label]\n",
    "        \n",
    "        x1, y1, x2, y2 = int(box[\"xmin\"]), int(box[\"ymin\"]), int(box[\"xmax\"]), int(box[\"ymax\"])\n",
    "        thickness = max(2, int(score * 8))\n",
    "        cv2.rectangle(img_vis, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        text = f\"{label} {score:.3f}\"\n",
    "        text_scale = 0.6\n",
    "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, text_scale, 2)[0]\n",
    "        cv2.rectangle(img_vis, (x1, y1-text_size[1]-10), (x1+text_size[0]+10, y1), color, -1)\n",
    "        cv2.putText(img_vis, text, (x1+5, y1-5), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   text_scale, (255, 255, 255), 2)\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img_vis, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"{title} - {len(filtered_dets)} objects (threshold ≥ {confidence_threshold:.2f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return filtered_dets\n",
    "\n",
    "def get_detection_summary(detections, confidence_threshold=0.1):\n",
    "    \"\"\"Get summary statistics of detections\"\"\"\n",
    "    filtered = [d for d in detections if d[\"score\"] >= confidence_threshold]\n",
    "    if not filtered:\n",
    "        return \"No detections above threshold\"\n",
    "    \n",
    "    label_counts = Counter([d['label'] for d in filtered])\n",
    "    summary = f\"Found {len(filtered)} detections:\\n\"\n",
    "    for label, count in label_counts.most_common():\n",
    "        avg_conf = np.mean([d['score'] for d in filtered if d['label'] == label])\n",
    "        summary += f\"  • {label}: {count} ({avg_conf:.3f} avg confidence)\\n\"\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive detector ready!\n"
     ]
    }
   ],
   "source": [
    "# Interactive Detection Interface\n",
    "class InteractiveDetector:\n",
    "    def __init__(self, image_paths, search_terms):\n",
    "        self.image_paths = image_paths\n",
    "        self.search_terms = search_terms\n",
    "        self.current_detections = {}\n",
    "        self.current_model = None\n",
    "        self.current_image_path = None\n",
    "        \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create interactive widgets\"\"\"\n",
    "        # Image selection dropdown\n",
    "        image_options = [(os.path.basename(path), path) for path in self.image_paths]\n",
    "        image_dropdown = widgets.Dropdown(\n",
    "            options=image_options,\n",
    "            value=self.image_paths[0],\n",
    "            description='Image:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Model selection dropdown\n",
    "        model_dropdown = widgets.Dropdown(\n",
    "            options=detector_manager.available_models,\n",
    "            value=detector_manager.available_models[0],\n",
    "            description='Model:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Threshold slider\n",
    "        threshold_slider = widgets.FloatSlider(\n",
    "            value=0.15,\n",
    "            min=0.01,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description='Confidence Threshold:',\n",
    "            readout_format='.3f',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Max detections slider\n",
    "        max_detections_slider = widgets.IntSlider(\n",
    "            value=50,\n",
    "            min=1,\n",
    "            max=200,\n",
    "            step=1,\n",
    "            description='Max Detections:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Run button\n",
    "        run_button = widgets.Button(\n",
    "            description='🔍 Run Detection',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='150px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Output area\n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def run_detection(button):\n",
    "            with output:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                image_path = image_dropdown.value\n",
    "                model_name = model_dropdown.value\n",
    "                threshold = threshold_slider.value\n",
    "                max_dets = max_detections_slider.value\n",
    "                \n",
    "                print(f\" Running {model_name} detection...\")\n",
    "                print(f\" Image: {os.path.basename(image_path)}\")\n",
    "                print(f\" Threshold: {threshold:.3f}, Max detections: {max_dets}\")\n",
    "                \n",
    "                try:\n",
    "                    # Load image\n",
    "                    image = cv2.imread(image_path)\n",
    "                    if image is None:\n",
    "                        print(f\" Error: Could not load image {image_path}\")\n",
    "                        return\n",
    "                    \n",
    "                    # Run detection\n",
    "                    detections = detector_manager.detect(model_name, image_path, self.search_terms)\n",
    "                    self.current_detections[model_name] = detections\n",
    "                    self.current_model = model_name\n",
    "                    self.current_image_path = image_path\n",
    "                    \n",
    "                    # Visualize results\n",
    "                    filtered_dets = visualize_detections(\n",
    "                        image, detections, \n",
    "                        title=f\"{model_name} Detection Results\",\n",
    "                        confidence_threshold=threshold\n",
    "                    )\n",
    "                    \n",
    "                    # Show summary\n",
    "                    print(\"📈 Detection Summary:\")\n",
    "                    print(get_detection_summary(detections, threshold))\n",
    "                    \n",
    "                    # Show detailed results if not too many\n",
    "                    if len(filtered_dets) <= 20:\n",
    "                        print(\"\\n📋 Detailed Results:\")\n",
    "                        for i, det in enumerate(sorted(filtered_dets, key=lambda x: x[\"score\"], reverse=True)[:max_dets]):\n",
    "                            box = det[\"box\"]\n",
    "                            print(f\"  {i+1:2d}. {det['label']:<20} {det['score']:.3f} \"\n",
    "                                  f\"[{box['xmin']:.0f},{box['ymin']:.0f},{box['xmax']:.0f},{box['ymax']:.0f}]\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\" Error: {e}\")\n",
    "        \n",
    "        # Connect button to function\n",
    "        run_button.on_click(run_detection)\n",
    "        \n",
    "        # Layout\n",
    "        controls = widgets.VBox([\n",
    "            widgets.HBox([image_dropdown, model_dropdown]),\n",
    "            widgets.HBox([threshold_slider, max_detections_slider]),\n",
    "            run_button\n",
    "        ])\n",
    "        \n",
    "        display(controls, output)\n",
    "        \n",
    "        return controls, output\n",
    "\n",
    "print(\"Interactive detector ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Detection Interface\n",
      "Available images: 2016 images\n",
      "  1. Explorer_HD2K_SN36949228_15-51-04_R.png\n",
      "  2. Explorer_HD2K_SN36949228_09-43-28_L.png\n",
      "  3. Explorer_HD2K_SN36949228_09-43-28_R.png\n",
      "  4. Explorer_HD2K_SN36949228_09-43-39_L.png\n",
      "  5. Explorer_HD2K_SN36949228_09-43-39_R.png\n",
      "  ... and 2011 more images\n",
      " Search terms: 28 terms\n",
      " Available models: ['google/owlvit-base-patch16', 'google/owlvit-base-patch32', 'google/owlvit-large-patch14', 'GroundingDINO']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0719180bfde44268969b4374e924065b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Image:', options=(('Explorer_HD2K_SN36949228_15-51-04_R.pn…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5579ce6e7cdc4345bfbd9a68c8837d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Instructions:\n",
      "1. Select an image from the dropdown\n",
      "2. Select a model from the dropdown\n",
      "3. Adjust the confidence threshold slider\n",
      "4. Set maximum detections limit\n",
      "5. Click 'Run Detection' to see results\n",
      "6. Try different images and models to compare performance!\n"
     ]
    }
   ],
   "source": [
    "IMAGE_PATHS = [\n",
    "    \"/home/ut-ai/ai-works/adaptibot/yolo_detect/processed/images/Explorer_HD2K_SN36949228_15-51-04_R.png\",\n",
    "]\n",
    "\n",
    "# Auto-discover more images in the processed/images directory\n",
    "processed_images_dir = \"/home/ut-ai/ai-works/adaptibot/yolo_detect/processed/images/\"\n",
    "if os.path.exists(processed_images_dir):\n",
    "    for filename in sorted(os.listdir(processed_images_dir)):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            full_path = os.path.join(processed_images_dir, filename)\n",
    "            if full_path not in IMAGE_PATHS:\n",
    "                IMAGE_PATHS.append(full_path)\n",
    "\n",
    "# Optimized search terms (focused on what's likely in the image)\n",
    "SEARCH_TERMS = [\n",
    "    # Generic terms (work best)\n",
    "    \"toy\", \"object\", \"container\", \"item\",\n",
    "    \n",
    "    # Specific objects\n",
    "    \"can\", \"soda can\", \"aluminum can\", \"beverage can\", \"cylinder\",\n",
    "    \"duck\", \"rubber duck\", \"toy duck\", \"yellow duck\", \"bath duck\",\n",
    "    \"cup\", \"mug\", \"glass\", \"drinking vessel\",\n",
    "    \"sponge\", \"cleaning sponge\", \"rectangular object\",\n",
    "    \"ball\", \"round ball\", \"sphere\", \"toy ball\",\n",
    "    \"vegetable\", \"fruit\", \"food item\"\n",
    "]\n",
    "\n",
    "print(f\"Interactive Detection Interface\")\n",
    "print(f\"Available images: {len(IMAGE_PATHS)} images\")\n",
    "for i, path in enumerate(IMAGE_PATHS[:5]):  # Show first 5\n",
    "    print(f\"  {i+1}. {os.path.basename(path)}\")\n",
    "if len(IMAGE_PATHS) > 5:\n",
    "    print(f\"  ... and {len(IMAGE_PATHS)-5} more images\")\n",
    "    \n",
    "print(f\" Search terms: {len(SEARCH_TERMS)} terms\")\n",
    "print(f\" Available models: {detector_manager.available_models}\")\n",
    "\n",
    "# Create and launch interactive detector\n",
    "interactive_detector = InteractiveDetector(IMAGE_PATHS, SEARCH_TERMS)\n",
    "controls, output = interactive_detector.create_interface()\n",
    "\n",
    "print(\"\\n Instructions:\")\n",
    "print(\"1. Select an image from the dropdown\")\n",
    "print(\"2. Select a model from the dropdown\")\n",
    "print(\"3. Adjust the confidence threshold slider\")\n",
    "print(\"4. Set maximum detections limit\")\n",
    "print(\"5. Click 'Run Detection' to see results\")\n",
    "print(\"6. Try different images and models to compare performance!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
